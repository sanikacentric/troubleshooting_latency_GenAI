{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Infrastructure/main.tf"
      ],
      "metadata": {
        "id": "ILUUyG4T7OPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQNaLE0I7JJT"
      },
      "outputs": [],
      "source": [
        "# infrastructure/main.tf\n",
        "\n",
        "terraform {\n",
        "  required_providers {\n",
        "    google = {\n",
        "      source  = \"hashicorp/google\"\n",
        "      version = \"~> 4.0\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "provider \"google\" {\n",
        "  project = var.project_id\n",
        "  region  = var.region\n",
        "}\n",
        "\n",
        "# Enable required APIs\n",
        "resource \"google_project_service\" \"apis\" {\n",
        "  for_each = toset([\n",
        "    \"run.googleapis.com\",\n",
        "    \"redis.googleapis.com\",\n",
        "    \"aiplatform.googleapis.com\",\n",
        "    \"pubsub.googleapis.com\"\n",
        "  ])\n",
        "  service = each.key\n",
        "}\n",
        "\n",
        "# Create a VPC network\n",
        "resource \"google_compute_network\" \"vpc_network\" {\n",
        "  name                    = \"chatbot-vpc\"\n",
        "  auto_create_subnetworks = false\n",
        "  depends_on              = [google_project_service.apis]\n",
        "}\n",
        "\n",
        "# Create a subnet for Redis\n",
        "resource \"google_compute_subnetwork\" \"subnet\" {\n",
        "  name          = \"chatbot-subnet\"\n",
        "  ip_cidr_range = \"10.0.0.0/16\"\n",
        "  region        = var.region\n",
        "  network       = google_compute_network.vpc_network.id\n",
        "}\n",
        "\n",
        "# Create Redis Instance (Cloud Memorystore)\n",
        "resource \"google_redis_instance\" \"chatbot_cache\" {\n",
        "  name               = \"chatbot-redis-cache\"\n",
        "  tier               = \"STANDARD_HA\"\n",
        "  memory_size_gb     = 5\n",
        "  region             = var.region\n",
        "  authorized_network = google_compute_network.vpc_network.id\n",
        "  connect_mode       = \"PRIVATE_SERVICE_ACCESS\"\n",
        "\n",
        "  depends_on = [google_project_service.apis]\n",
        "}\n",
        "\n",
        "# Create Pub/Sub Topic for async embedding requests\n",
        "resource \"google_pubsub_topic\" \"embedding_requests\" {\n",
        "  name = \"query-embedding-requests\"\n",
        "}\n",
        "\n",
        "# Deploy Cloud Run Service\n",
        "resource \"google_cloud_run_service\" \"chatbot_api\" {\n",
        "  name     = \"chatbot-api\"\n",
        "  location = var.region\n",
        "\n",
        "  template {\n",
        "    spec {\n",
        "      containers {\n",
        "        image = \"gcr.io/${var.project_id}/chatbot-api:latest\" # Image will be built and pushed during deployment\n",
        "\n",
        "        ports {\n",
        "          container_port = 8080\n",
        "        }\n",
        "\n",
        "        # Environment variables for application configuration\n",
        "        env {\n",
        "          name  = \"REDIS_HOST\"\n",
        "          value = google_redis_instance.chatbot_cache.host\n",
        "        }\n",
        "        env {\n",
        "          name  = \"REDIS_PORT\"\n",
        "          value = google_redis_instance.chatbot_cache.port\n",
        "        }\n",
        "        env {\n",
        "          name  = \"GOOGLE_CLOUD_PROJECT\"\n",
        "          value = var.project_id\n",
        "        }\n",
        "        env {\n",
        "          name  = \"EMBEDDING_MODEL_ID\"\n",
        "          value = \"textembedding-gecko@003\"\n",
        "        }\n",
        "        env {\n",
        "          name  = \"PREDICTION_ENDPOINT_ID\"\n",
        "          value = var.vertex_ai_endpoint_id # You would create this separately or via Terraform\n",
        "        }\n",
        "        env {\n",
        "          name  = \"EMBEDDING_TOPIC_ID\"\n",
        "          value = google_pubsub_topic.embedding_requests.name\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  traffic {\n",
        "    percent         = 100\n",
        "    latest_revision = true\n",
        "  }\n",
        "\n",
        "  depends_on = [google_project_service.apis]\n",
        "}\n",
        "\n",
        "# Allow unauthenticated access to Cloud Run service (can be restricted later)\n",
        "data \"google_iam_policy\" \"noauth\" {\n",
        "  binding {\n",
        "    role = \"roles/run.invoker\"\n",
        "    members = [\n",
        "      \"allUsers\",\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "\n",
        "resource \"google_cloud_run_service_iam_policy\" \"noauth\" {\n",
        "  service     = google_cloud_run_service.chatbot_api.name\n",
        "  location    = google_cloud_run_service.chatbot_api.location\n",
        "  policy_data = data.google_iam_policy.noauth.policy_data\n",
        "}\n",
        "\n",
        "# --- CLOUD LOAD BALANCER CONFIGURATION ---\n",
        "\n",
        "# Reserve a global static IP\n",
        "resource \"google_compute_global_address\" \"chatbot_lb_ip\" {\n",
        "  name = \"chatbot-lb-ip\"\n",
        "}\n",
        "\n",
        "# Create a serverless network endpoint group (NEG) for Cloud Run\n",
        "resource \"google_compute_region_network_endpoint_group\" \"chatbot_neg\" {\n",
        "  name                  = \"chatbot-neg\"\n",
        "  region                = var.region\n",
        "  cloud_run {\n",
        "    service = google_cloud_run_service.chatbot_api.name\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create a backend service for the load balancer\n",
        "resource \"google_compute_backend_service\" \"chatbot_backend\" {\n",
        "  name                  = \"chatbot-backend\"\n",
        "  protocol              = \"HTTP\"\n",
        "  port_name             = \"http\"\n",
        "  load_balancing_scheme = \"EXTERNAL_MANAGED\"\n",
        "  timeout_sec           = 30\n",
        "  enable_cdn            = false\n",
        "\n",
        "  backend {\n",
        "    group = google_compute_region_network_endpoint_group.chatbot_neg.id\n",
        "  }\n",
        "\n",
        "  # Health check for the backend service\n",
        "  health_checks = [google_compute_health_check.chatbot_hc.id]\n",
        "}\n",
        "\n",
        "# Health check configuration (points to /healthz endpoint)\n",
        "resource \"google_compute_health_check\" \"chatbot_hc\" {\n",
        "  name = \"chatbot-health-check\"\n",
        "  http_health_check {\n",
        "    port_specification = \"USE_SERVING_PORT\"\n",
        "    request_path       = \"/healthz\"\n",
        "    check_interval_sec = 5\n",
        "    timeout_sec        = 3\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create a URL map to route all requests to our backend\n",
        "resource \"google_compute_url_map\" \"chatbot_url_map\" {\n",
        "  name            = \"chatbot-url-map\"\n",
        "  default_service = google_compute_backend_service.chatbot_backend.id\n",
        "}\n",
        "\n",
        "# Create an SSL certificate (Google-managed)\n",
        "resource \"google_compute_managed_ssl_certificate\" \"chatbot_ssl\" {\n",
        "  name = \"chatbot-ssl-cert\"\n",
        "  managed {\n",
        "    domains = [\"chatbot.${var.domain_name}.\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create an HTTPS target proxy\n",
        "resource \"google_compute_target_https_proxy\" \"chatbot_https_proxy\" {\n",
        "  name    = \"chatbot-https-proxy\"\n",
        "  url_map = google_compute_url_map.chatbot_url_map.id\n",
        "  ssl_certificates = [\n",
        "    google_compute_managed_ssl_certificate.chatbot_ssl.id\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Create the global forwarding rule\n",
        "resource \"google_compute_global_forwarding_rule\" \"chatbot_https_rule\" {\n",
        "  name                  = \"chatbot-https-rule\"\n",
        "  ip_protocol           = \"TCP\"\n",
        "  port_range            = \"443\"\n",
        "  load_balancing_scheme = \"EXTERNAL_MANAGED\"\n",
        "  ip_address            = google_compute_global_address.chatbot_lb_ip.address\n",
        "  target                = google_compute_target_https_proxy.chatbot_https_proxy.id\n",
        "}\n",
        "\n",
        "# DNS record to point your domain to the Load Balancer IP\n",
        "resource \"google_dns_record_set\" \"chatbot_dns\" {\n",
        "  name         = \"chatbot.${var.domain_name}.\"\n",
        "  type         = \"A\"\n",
        "  ttl          = 300\n",
        "  managed_zone = var.dns_zone_name\n",
        "  rrdatas      = [google_compute_global_address.chatbot_lb_ip.address]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Application Code (application/main.py)\n",
        "This is the complete, updated Flask application that runs on Cloud Run."
      ],
      "metadata": {
        "id": "6cS8BtCS7X9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# application/main.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import hashlib\n",
        "import re\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded\n",
        "\n",
        "import redis\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Import Google Cloud clients\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import pubsub_v1\n",
        "\n",
        "# Initialize Flask App\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- GCP Clients Initialization ---\n",
        "# Initialize Cloud Memorystore (Redis) client\n",
        "redis_client = redis.Redis(\n",
        "    host=os.environ.get('REDIS_HOST', 'localhost'),\n",
        "    port=int(os.environ.get('REDIS_PORT', 6379)),\n",
        "    decode_responses=True\n",
        ")\n",
        "\n",
        "# Initialize Vertex AI\n",
        "aiplatform.init(\n",
        "    project=os.environ.get('GOOGLE_CLOUD_PROJECT'),\n",
        "    location=os.environ.get('VERTEX_AI_LOCATION', 'us-central1')\n",
        ")\n",
        "# Initialize the Vertex AI Text Embedding Model\n",
        "embedding_model = aiplatform.models.Model(os.environ.get('EMBEDDING_MODEL_ID', 'textembedding-gecko@003'))\n",
        "# Initialize the Vertex AI Text Generation Model endpoint\n",
        "prediction_endpoint = aiplatform.Endpoint(os.environ.get('PREDICTION_ENDPOINT_ID', ''))\n",
        "\n",
        "# Initialize Pub/Sub publisher for batch embedding processing\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "EMBEDDING_PUBSUB_TOPIC = publisher.topic_path(\n",
        "    os.environ.get('GOOGLE_CLOUD_PROJECT'),\n",
        "    os.environ.get('EMBEDDING_TOPIC_ID', 'query-embedding-requests')\n",
        ")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def normalize_query(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize the user query to increase cache hit rate.\n",
        "    \"\"\"\n",
        "    query = query.lower().strip()\n",
        "    query = re.sub(r'[^\\w\\s]', '', query)\n",
        "    words = query.split()\n",
        "    stopwords = {'what', 'is', 'the', 'a', 'an', 'how', 'to', 'do', 'i', 'can', 'you'}\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def get_query_hash(normalized_query: str) -> str:\n",
        "    \"\"\"Generate a unique hash for the normalized query to use as the Redis key.\"\"\"\n",
        "    return hashlib.sha256(normalized_query.encode()).hexdigest()\n",
        "\n",
        "@retry(\n",
        "    retry=retry_if_exception_type((ResourceExhausted, DeadlineExceeded)),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "    stop=stop_after_attempt(3),\n",
        "    reraise=True\n",
        ")\n",
        "def call_vertex_ai_llm(context: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls the optimized Vertex AI LLM endpoint with retry logic.\n",
        "    \"\"\"\n",
        "    instance = {\n",
        "        \"context\": context,\n",
        "        \"message\": question\n",
        "    }\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"maxOutputTokens\": 256,\n",
        "        \"topP\": 0.8,\n",
        "        \"topK\": 40\n",
        "    }\n",
        "    try:\n",
        "        response = prediction_endpoint.predict(\n",
        "            instances=[instance],\n",
        "            parameters=parameters\n",
        "        )\n",
        "        return response.predictions[0]['content']\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error calling Vertex AI LLM: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- API Routes ---\n",
        "@app.route('/healthz', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint for Load Balancer and Monitoring.\"\"\"\n",
        "    return jsonify({\"status\": \"ok\"}), 200\n",
        "\n",
        "@app.route('/api/chat', methods=['POST'])\n",
        "def chat_handler():\n",
        "    \"\"\"\n",
        "    Main endpoint for user chat queries.\n",
        "    \"\"\"\n",
        "    request_data = request.get_json()\n",
        "    user_query = request_data.get('query', '')\n",
        "\n",
        "    if not user_query:\n",
        "        return jsonify({\"error\": \"Query is required\"}), 400\n",
        "\n",
        "    logger.info(f\"Received query: {user_query}\")\n",
        "\n",
        "    normalized_query = normalize_query(user_query)\n",
        "    query_hash = get_query_hash(normalized_query)\n",
        "\n",
        "    cached_response = redis_client.get(f\"response:{query_hash}\")\n",
        "    if cached_response:\n",
        "        logger.info(\"Cache HIT for response\")\n",
        "        return jsonify({\"response\": cached_response, \"source\": \"cache\"})\n",
        "\n",
        "    cached_embedding = redis_client.get(f\"embedding:{query_hash}\")\n",
        "    if cached_embedding:\n",
        "        logger.info(\"Cache HIT for embedding\")\n",
        "        relevant_context = \"[Context retrieved from Vector DB using cached embedding]\"\n",
        "    else:\n",
        "        logger.info(\"Cache MISS for embedding\")\n",
        "        data = {\"query\": user_query, \"normalized_query\": normalized_query, \"hash\": query_hash}\n",
        "        future = publisher.publish(EMBEDDING_PUBSUB_TOPIC, json.dumps(data).encode(\"utf-8\"))\n",
        "        logger.info(f\"Published query for async embedding: {future.result()}\")\n",
        "        relevant_context = \"[General context about products and regulations]\"\n",
        "\n",
        "    try:\n",
        "        llm_response = call_vertex_ai_llm(relevant_context, user_query)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": \"Our AI service is temporarily busy. Please try again.\"}), 503\n",
        "\n",
        "    redis_client.setex(f\"response:{query_hash}\", 86400, llm_response)\n",
        "    logger.info(\"Successfully generated and cached response.\")\n",
        "    return jsonify({\"response\": llm_response, \"source\": \"vertex-ai\"})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=False, host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))"
      ],
      "metadata": {
        "id": "sOKH8Unc7Y91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}